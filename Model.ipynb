{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6028aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint, HuggingFaceEndpointEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import load_prompt\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "61baab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id = \"google/gemma-2-2b-it\",\n",
    "    task='text-generation',\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "embed_loader = \"Qwen/Qwen3-Embedding-8B\"\n",
    "embed_model = HuggingFaceEndpointEmbeddings(client=embed_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d89c5594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loaders\n",
    "file_path = \"/Users/gyan/Desktop/Faq/ugrulebook_2.pdf\"\n",
    "loader_ = PDFPlumberLoader(file_path)\n",
    "docs = loader_.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5780d878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREFACE\\nThe Indian Institute of Technology Bombay (IITB) is one of the Indian Institutes of Technology in\\nthe country, set up with the objective of conducting research, imparting education, and training\\nin various fields of Science and Technology. The Institute is now recognized as a leader in science\\nand engineering education worldwide. It has established a firm foundation for education and\\nresearch with a vision to be the fountainhead of new ideas and innovations in technology and\\nscience. The mission of IIT Bombay is to create an ambiance in which new ideas, research and\\nscholarship flourish and from which the leaders and innovators of tomorrow emerge.\\nIIT Bombay on an average annually admits more than 1000 candidates for the undergraduate\\nprogrammes (B.Tech./ Dual Degree and B.S.) through the Joint Entrance Examination (JEE), more\\nthan 30 candidates for B.Des. Programme through the Undergraduate Common Entrance Exam\\nfor Design (UCEED), around 300 candidates for the M.Sc. and the M.Sc. Ph.D. Dual Degree pro-\\ngrammes, more than 1000 candidates for postgraduate programmes and around 300 candidates\\nfor Ph.D. programmes. IIT Bombay follows a credit-based semester system. There are two se-\\nmesters in an academic year (Autumn: July- Nov; and Spring: January-April) during which all\\ncourses specified for various degree programmes are offered. There is one additional term dur-\\ning summer, called the Summer Term, for summer courses. Students are required to meet spec-\\nified academic requirements each semester.\\nThis booklet provides comprehensive information on the Rules and Regulations for B.Tech., Dual\\nDegree, B.S. and B.Des. Programmes.\\nIIT Bombay gives freedom to its various academic units to tailor their academic programmes as\\nper their specific needs, within the frame work of the Rules and Regulations approved by the\\nSenate from time to time. Keeping in view the new technical developments and to allow stu-\\ndents some freedom to explore topics of their interest, IIT Bombay revised its Undergraduate\\nProgrammes curriculum (w.e.f. the Academic Year 2013). The revised curriculum, provides ad-\\nditional opportunities and flexibility for students to optimize their learning experience. This\\nneeds continuous and meticulous planning of the academic profile on the part of each student\\nto fully utilize the opportunities. The students, and parents/ guardians, are therefore, advised in\\ntheir own interest to get fully familiar with the Academic system of the Institute. Studentâ€™s at-\\ntention is brought particularly to the assessment procedures and the specific rules governing\\nthe grading system, academic performance requirements etc.\\nIIT Bombay offers its students best opportunities that are needed to create outstanding scien-\\ntists and engineers. We hope that you will strive hard to utilize the resources available to fully\\ndevelop your potential as an engineer/ scientist/ designer /economist/ social scientist.\\nWe wish all our students a very bright future and a successful career.\\nDean of Academic Programmes\\n6\\nMove to Index\\n'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[5].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d891827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove \\n from text\n",
    "def remove_newlines(text_list):\n",
    "    return [text.replace('\\n', ' ') for text in text_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "88f364ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n"
     ]
    }
   ],
   "source": [
    "# Text Splitters\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "\n",
    "all_text = \"\\n\".join(doc.page_content for doc in docs)\n",
    "all_text = remove_newlines([all_text]) # removing '\\n'\n",
    "chunks = splitter.create_documents(all_text)\n",
    "print(len(chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dee54b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c8560328-d735-4249-82fe-590d7a28b567',\n",
       " '3240272d-2206-4b51-a043-7dc1d90ca269',\n",
       " '10649f6d-73a6-49e6-a666-7759c0353785',\n",
       " '61d6b167-cedf-4cb5-8247-311765409e95',\n",
       " '98105512-96fb-4660-92d4-6870ec929b4f',\n",
       " '71643f77-41a8-40bc-8b4e-eba251a74934',\n",
       " '792be6e2-b374-43ce-9baf-e0b1860fa70b',\n",
       " 'be076dfb-ad8e-455d-836e-7b84025e77ad',\n",
       " 'bc60791c-0730-4aa2-b9bd-59eaec2cffd7',\n",
       " '94520547-bc6d-444b-8a2d-aaa21abf11cf',\n",
       " '416d9280-941a-4f76-9fe2-acc1de46e2fa',\n",
       " '7bbac4e9-1977-4d6d-88d5-35c6456778bb',\n",
       " '48d088ad-9a71-42f0-b2ba-f1d205d86211',\n",
       " '95e0b99f-2250-4c37-b1a6-48e1e87821bb',\n",
       " 'a341f062-3fec-4578-b433-b37acfbfc278',\n",
       " '09a351c0-92b4-41e6-a52f-8bb2dbe52561',\n",
       " '0ee8ae3a-4242-4373-8dcc-c404f5a237b8',\n",
       " '03fcc2e2-0469-4bc7-b5cc-0fb6eef70d61',\n",
       " 'a17a1b05-09dc-47b5-9a55-0ebccacb7d14',\n",
       " '4cbec01f-cce7-4656-9b6a-d9e26f125d96',\n",
       " '5bc0963e-b9e6-4a48-99ea-f58a4094b5b2',\n",
       " '170a3906-d0e2-4b7f-acd3-81f10c3b6c2c',\n",
       " 'fc736a23-9ec8-44a7-b06c-3dbcd3dd3b82',\n",
       " '9ccb9907-27dc-486f-af76-a1a1593c1328',\n",
       " '2765525e-146d-4275-9fda-99ce9f059dc7',\n",
       " '83fc0399-eeb9-4df0-93b5-e6b6fbcfbf6b',\n",
       " '697fee98-0513-438c-9bac-89be77e49e40',\n",
       " '8af3a4f4-a00a-4a50-a6bb-73c00f5a105d',\n",
       " '2ed7f89a-8ac0-4ff0-99b8-9746ee71aef1',\n",
       " '1cd486e4-d68b-4622-b914-3b713ae937b9',\n",
       " '0910a527-d3ac-4528-9a2b-bff99c3d7dfe',\n",
       " '916b77d9-c1b7-4a05-81b4-3190f05d6faa',\n",
       " 'ce00a044-669a-4b58-9fe4-fa34b389102b',\n",
       " 'aaae6aee-9ab9-4140-b850-adb52515e117',\n",
       " '058d52f6-0936-46b4-bdb3-f692c74b878f',\n",
       " '068a7c8c-5f26-4713-a9ac-3470257962f6',\n",
       " '0cfa0347-df85-4cc2-8875-464873eb0cc5',\n",
       " '6365c6ea-a2cd-46fe-8fcb-71d5010daf86',\n",
       " '7ea9e4d2-bf27-4cf8-a087-d1c5ad968414',\n",
       " '3e78de12-bf39-4610-90ff-a056b0f2c652',\n",
       " 'f37ecf1e-8a9b-4b9d-97ca-25a80b0ab097',\n",
       " '7d6def7a-b187-4b45-9ef3-8dd9aa300f46',\n",
       " '95636df1-2334-4277-a74e-6868b72d5452',\n",
       " '35e7e92a-a49a-4b46-ab98-22115b782bdf',\n",
       " 'd08ec553-65bd-4f79-9ac4-a775d5645d8c',\n",
       " '394ca483-2041-4ba0-94ad-0f5961b6c440',\n",
       " '95209f52-05d8-4763-b450-108c0d356e6a',\n",
       " '223d762e-5597-4432-befa-653dcd2317cb',\n",
       " '4f5cf536-0747-48fd-a926-afaf46f58eb0',\n",
       " '02f5aa94-c1de-4058-9e24-cf87d8dd601a',\n",
       " 'ce1a7cc6-de60-400c-a76b-a9d61fb60aea',\n",
       " '341c66bc-5a5a-42b7-babf-1640c91925e8',\n",
       " '0a026494-1500-45e6-ad09-8916c7cb7a3b',\n",
       " 'bea7b8d6-771b-4097-8f24-79f9d67e9e26',\n",
       " '750b10e9-831b-474a-b6d7-6fc7b40e9720',\n",
       " 'c5ef163a-b977-48fe-a0c8-85339ddda59d',\n",
       " 'f7cc39bd-9bdd-457a-9b37-457b724e3e3b',\n",
       " '6e269b17-c015-4c2a-9e26-101e87c859f9',\n",
       " '96f7c169-4602-4198-b9e1-17ec3db91e5d',\n",
       " '8adcb01d-ddd6-4f3a-9f5d-b932451e952e',\n",
       " 'fa45ada2-4815-473f-a958-fea25fba1c75',\n",
       " '5faeda9d-4fbc-4a5f-8f30-cbd232931c1f',\n",
       " '8843b770-8e34-499d-b0db-114d2cb62f16',\n",
       " '932c0efc-90da-4224-8e1e-ec59e437edaa',\n",
       " '0fe0dcc1-1559-472b-ac04-256afece418d',\n",
       " '70337c72-2d37-4c5c-82d2-f10662cf104e',\n",
       " '40e6aa67-a1ac-4ecc-9738-4c8cb188fd4c',\n",
       " 'f7ac7aa5-76ce-4da3-b302-09e08af34163',\n",
       " 'a8bf78ae-74f5-4c57-826c-42f1208dd24a',\n",
       " '9732bc6f-7fc2-4ba8-8aa3-ffb41796eda7',\n",
       " 'ace5131f-30e5-4215-94ce-d8a024754504',\n",
       " '0559f9f9-9393-458d-a5d0-559494a7aef5',\n",
       " '0b9c2490-9c50-418f-9762-0e38d0909a89',\n",
       " '8b0aad6b-c7fe-4fea-b8d0-855805335bbf',\n",
       " 'c79692fa-e4b4-4880-bb51-27c41eac3947',\n",
       " 'fcd8ba50-8b94-4dc8-ad21-63d4fc214059',\n",
       " '11a1dc92-263c-4328-9342-402399a8e146',\n",
       " '30b7ca06-9647-4381-9654-1df68481eb93',\n",
       " '71ce47bd-d11b-4319-9616-71cd302b4113',\n",
       " 'cc745d79-29ca-4f70-9ff9-35a3e713edd1',\n",
       " 'befb5645-4231-4814-9e69-6bb22d5ec67d',\n",
       " 'de4c1ee4-c930-444d-b0b7-0fc522644be2',\n",
       " '477bb58f-7062-4f5f-abb9-08748550b670',\n",
       " '4b34a514-c20b-4420-9724-c6eba0040523',\n",
       " '0385c1f7-a8ec-438d-9279-797e5ece822f',\n",
       " '971169d6-af37-4981-b947-72f5b7315ecc',\n",
       " 'f3a67021-2034-4fd4-927b-b67dd06666a9',\n",
       " 'd785900d-4956-481b-9ba4-c8804d7cd199',\n",
       " '65ea8889-ee5f-41d8-8c67-9cb51a205e32',\n",
       " 'bf2dea1e-1f7e-4364-96d2-58ef7db6088b',\n",
       " 'c2802fae-9cd5-498b-9015-8117b33e459c',\n",
       " 'ea2003cb-a4cd-466e-9ea3-b7253c805696',\n",
       " '1a5785ff-3152-481b-b9dc-085c3df3fed2',\n",
       " '9e2ede80-8b1b-48a3-ba79-dc569752f6aa',\n",
       " 'c9a7c43e-89c0-4da1-a1de-c2a769c739d7',\n",
       " '11e057a8-cb45-4e12-bb25-20be7d48019e',\n",
       " 'cd99e878-d68d-42d1-815d-c6ea9b3cdb49',\n",
       " 'b5fb6c88-ab02-4b40-8278-51a6e0fe90df',\n",
       " '1a31603e-8d0f-40bf-844c-7008728c156e',\n",
       " '984d6e4f-3237-45b4-bc5f-ae8c7656d4ec',\n",
       " '463ab888-0f94-4806-9a5f-b0f5297d74d6',\n",
       " 'a096bd46-421f-4125-82dc-425668ccd2e8',\n",
       " 'ea0f486f-cc67-4b06-83cb-231bd71023fe',\n",
       " 'bb18559d-a86c-4e6d-869f-4200d5e39d1d',\n",
       " '0a2619cc-79c1-455a-aeb8-92b9fe307632',\n",
       " 'f0e03359-0c5a-4ca8-85e4-e059161106cf',\n",
       " '45d1db24-5e29-47d4-91df-f0d553e5a79f',\n",
       " '9c42c928-2c3a-4f7f-93c1-1d7229b5553f',\n",
       " '689b52e9-8f9f-4374-bb76-93a0de702fa5',\n",
       " '01bf2afa-afc5-420d-8eb0-396a99d48f9d',\n",
       " 'dce87105-84c8-498a-a3ef-32f49647774d',\n",
       " 'fab97f07-fbf7-4be5-94c3-3e7cf26b544b',\n",
       " '10c23b30-0e9f-4115-91cc-6bb010490c4b',\n",
       " '1bb0d199-2865-4195-96f5-be60a7863b98',\n",
       " '29b74cb3-bde1-4202-981b-2775c426de0a',\n",
       " '35a2a47c-21fb-4d70-9ee4-a092696f995a',\n",
       " 'a6722cfc-bb98-4022-84a7-8acbade838da',\n",
       " '7a2c37d5-b83a-4721-a082-ab9648822f55',\n",
       " '3afedf36-a5b6-4b14-aaec-4e14669b9652',\n",
       " 'dba9c515-3c0e-4efb-a562-b947c117fbcb',\n",
       " '289c5098-9eea-4fce-a13a-490a3eca6f2c',\n",
       " '0766e382-71a5-445f-83e5-8097d0c8d099',\n",
       " '17cc6f1d-ee23-4da3-b99a-7370b2b3f997',\n",
       " '7a62d83d-e07a-4cde-bb67-b833a474d3a5',\n",
       " '3696bf98-bfbc-4b17-9ec5-5c4e47e802af',\n",
       " '8b88ec24-d52d-422b-a820-d9c7523ca23f',\n",
       " '007d488f-c9a6-48c8-81df-873182b360b4',\n",
       " '3356a86e-8161-463e-8ddd-799f46ecb2ac',\n",
       " '66a2d7b3-2996-4ee7-b9bc-ee7cd0ab9219',\n",
       " '451e0442-902b-4565-bc4b-c5c4bb91b287',\n",
       " '91fb2abb-af2f-4dce-95a8-6a8a60b23c1f',\n",
       " 'a97c5692-4aaf-4771-b9c4-e2cebee9d3e1',\n",
       " '74d8bcfd-866f-4ffb-ad38-e4c0ab46366d',\n",
       " '38c423bc-3114-4de0-8848-de05e0bfdf91',\n",
       " '6414a38b-a144-4152-98f8-79de4a964447',\n",
       " 'bd31954e-3220-4a55-944b-679804033269',\n",
       " '553cc9b4-e35a-4c2e-aafc-bc88847532b9',\n",
       " '086cde35-0401-42d1-8da3-dfb8d4735c3e',\n",
       " '5f0f6477-ebe8-48a4-bc7e-9043c70d2559',\n",
       " '556bd3d1-cde4-4ea1-9cfb-3133bd38bd00',\n",
       " '3bcc0678-375f-47ad-b490-00ffd86886c6',\n",
       " '780def91-ba7e-4a6d-82ad-647c29beb007',\n",
       " 'b8654471-2a82-452e-a80a-142b34446e0a',\n",
       " 'b2a74f14-70af-4132-986b-c973af08398f',\n",
       " 'cd4e6629-f6fe-4c10-9014-6e1674dc5307',\n",
       " 'd68b8241-c307-402e-a34c-f897d4ffb63b',\n",
       " '4e0a2d1b-6854-48dd-921e-63921bacc3f2',\n",
       " 'cb01b3a9-50d2-4517-8734-5f582ef9b7a2',\n",
       " 'cdcef34b-ccdb-426b-9583-58f94be2ac11',\n",
       " 'b807030f-eee5-4e48-a3ca-f6d273700fdb',\n",
       " 'f26ccf3d-0a82-4a21-9e58-7e5b888ff704',\n",
       " 'b7236539-8352-4471-bc6d-0dc994f5bfef',\n",
       " 'e7ebe6c0-26b9-466c-b21b-d38043f27e67',\n",
       " 'adef1c8d-c600-484d-b1ef-357aad28ac4a',\n",
       " 'bdf59725-e89d-44d0-9a4a-e848ee6e1b65',\n",
       " '5f25e50f-6189-4b50-966a-6e51fe31f9b4',\n",
       " 'd978983e-7e2a-4d01-bb6f-156632d56cac',\n",
       " '7d8c2073-1fb9-47be-8287-b7bd9e011f9d',\n",
       " 'b338bfcc-c229-484e-89a5-2365a68ea974',\n",
       " '556f61b8-3f9c-4b18-943d-89f761ff4fc3',\n",
       " 'c4412ccd-f29d-4a99-9073-c71da92241cb',\n",
       " '0821ed6b-09f8-4d38-8c16-d8065ca21532',\n",
       " '0b111b5a-a980-42e3-a6c0-5d2731adae51',\n",
       " 'c3a5c0bb-0dfc-4b85-8b05-e86b8fd97e50',\n",
       " '04fe7778-76f0-4cb2-8653-c708c50c1219',\n",
       " '99d3cd00-82d8-41fb-859d-e57195fb30b3',\n",
       " '6635b500-fdf1-4c70-acb7-b2a4d6b1d195',\n",
       " '314a52fc-5e5d-40b3-9382-34307d677bb7',\n",
       " '692e7851-dbe6-431a-8067-51299e9c995b',\n",
       " 'd9df618b-f0a2-47f0-9f5a-764c21571151',\n",
       " '9950972a-0a57-44ea-b14d-11e15f513c56',\n",
       " 'c6473226-3782-4e65-ba8a-385534b0b84f',\n",
       " '4ea2cb75-e1a2-46e4-8fd0-34d3f59a1a8a',\n",
       " '384060a5-cde1-49e7-8eee-acbd7713a0cd',\n",
       " '4003307d-1a5f-4812-a8b3-ad2ccce8227d',\n",
       " 'c36f5e74-12cd-4f8e-9f11-cd0e1e545486',\n",
       " 'b5ea5992-e0c9-4587-9fb4-f1390a65e29a',\n",
       " '49165192-500e-4d76-8971-cf615cf44fbb',\n",
       " 'f3cc8fba-02ef-4396-90d5-bb87425dda8b',\n",
       " '34ebd8e1-167a-4bd5-88f4-650ecedad652',\n",
       " '034e4fd3-5d08-45d3-8181-e4a16df64706',\n",
       " 'e22b5312-8761-4971-a9ce-015a35963804',\n",
       " '3d319266-1b80-4bd1-aae7-92cfb2e37d7d',\n",
       " 'b000ce80-49b7-4315-8acb-0c1db693ffd4',\n",
       " 'ae401e96-41df-4f9d-a5c0-d527fe19647b',\n",
       " 'e9b423f6-f610-4175-848f-c97f6a9e699e',\n",
       " '1d45426e-fa79-4b9c-8808-166ee82ea3ab',\n",
       " '1a158364-b97a-43bd-b55b-ad454b98a2b1',\n",
       " 'de04720f-a49b-499c-8d38-fec563061e7a',\n",
       " 'cb242941-31c9-4f81-a416-6a88175371d1',\n",
       " '106dcceb-3cd1-4c73-9e61-3bf84b378940',\n",
       " 'e08a3d55-9e43-47a6-adef-b9ccdbf88e0e',\n",
       " '7c3397c7-c1df-409f-8124-9bb539c78e87',\n",
       " '67fab33d-57bb-4382-9534-72f8e250fc9e',\n",
       " 'c2ff3b76-c5e4-4128-a971-5c3dab9b3efa',\n",
       " '99aee0a8-c700-4008-8910-60a95b4afb98',\n",
       " '3d8c232d-2210-4028-aaed-b048a683577d',\n",
       " 'd2a5b1f6-63cb-4954-99c8-61c065ac940d',\n",
       " '2b36bdd3-95bc-4afd-83fe-7b5edc8d4b10',\n",
       " '38bcaef5-93ad-4b05-a462-92378452f273',\n",
       " 'c8323876-7c05-46f1-a876-4cd00b41b439',\n",
       " '3a3ccb73-c6ec-4b58-9d27-949b2ae2966a',\n",
       " '9657f3b6-497b-4f5b-9a67-ad96f51c5cb9',\n",
       " '1224dcaf-3781-4473-8ac9-92c6816f1f1f',\n",
       " 'ad228634-ea9d-4567-9835-2d04c47bce8e',\n",
       " '15f8b690-ebdb-41b8-9fe0-b2ea1f6c246f',\n",
       " 'e7a9faf1-0db6-4baa-80f9-63404609d972',\n",
       " '449f3c53-bf7f-49aa-a52b-f5c3de643133',\n",
       " 'de11be59-a672-4b7c-affb-9191507cb8a3',\n",
       " '4a72975c-3671-4316-b7b2-7f0981a12136',\n",
       " '2f407f03-847d-4199-aaf4-2ba349a63a11',\n",
       " 'dae9eac8-f4a3-4a22-9600-13986f1b5229',\n",
       " '27ae0312-59df-4bb3-b6ef-5f5cb876d9a2',\n",
       " '8f541c0e-6b07-4df1-b337-ca4d757c7c51',\n",
       " '737f5b6b-7a3d-44dc-a2dd-c7fd19edab31',\n",
       " 'ae7e5924-d42c-4198-8c75-0bdadeb21edb',\n",
       " 'e9783c09-5f16-4c47-94fa-b615910b03c8',\n",
       " 'ab61fe81-0e03-4925-8a3e-9192d019784e',\n",
       " '76849408-3665-4810-97f3-6e08787379c0',\n",
       " '0aaf4dae-87bc-499f-bff7-41455771fbc9',\n",
       " '005f809c-48c8-4342-a4b7-1624a43ad08a',\n",
       " 'c9978de1-d0b8-4111-bda5-f6fb0179a790',\n",
       " '3233d38b-4582-4b17-8e10-4a12d57ac4df',\n",
       " '13d6d726-e71c-4934-85e9-9373a723a95e',\n",
       " '3362cd88-e8d0-46da-b5fe-996b241967db',\n",
       " 'f4ab9ed8-e5d8-4704-bed5-d5123f3ff38a',\n",
       " '39c91ac0-56c1-4706-bf0d-bd2618b871ae',\n",
       " 'c6427ec2-39ab-4869-9f4b-e0c548f2c67d',\n",
       " '5acc1ddc-55cd-459d-a994-0530ab977a58',\n",
       " '220fd615-960c-428f-a838-2dc6097143e9',\n",
       " '18da89a0-74e0-47b2-814b-435d600347f6',\n",
       " '83211626-6566-4b59-99f2-80ddff4e35e7',\n",
       " 'aa046c9d-74d3-4491-afaf-72f3fe8c9aab',\n",
       " 'ba563bf1-91d9-4df4-860b-d7b9e6372ee2',\n",
       " 'd2b6a22f-2036-4c50-889b-e5a1bc00c5ca',\n",
       " 'f8a07751-4b28-40f1-afff-c8aec707fd38',\n",
       " '5bcf7559-41f3-40d5-8146-c34888f69204',\n",
       " 'b7651d41-4df3-4791-8692-388fc194dd4c',\n",
       " '2b830ac2-d45c-4541-84da-59ead070272c',\n",
       " '5f3c08b2-fb38-4a58-8de7-b8dae616679e',\n",
       " '409363be-d755-4c05-a1d4-a45e765b6fbe',\n",
       " 'e35a1497-3a54-44c8-92a4-4ad2080e7130',\n",
       " '3dbde544-a45a-4dd4-910c-d1b76de00173',\n",
       " '7463388a-d261-47c5-af9e-6c100aeb6864',\n",
       " '773ffbec-1243-46e3-a014-fd217cac67c1',\n",
       " '266aaf1e-a8d4-4d89-b837-9694335837d6',\n",
       " '8cecbef2-a9ff-41cd-b525-25b65a212713',\n",
       " '703a0a4b-a6ab-4d42-b237-272e691bc526',\n",
       " '6f8f1475-22a2-4aa7-aaf0-e75774f1655b',\n",
       " '86cc8c72-643e-4bee-8fb7-dfaefde8165e',\n",
       " '9cc37523-e98a-4750-a30f-fa67ee11cd29']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector Store\n",
    "vector_store = Chroma(\n",
    "    embedding_function=embed_model,\n",
    "    persist_directory=\"Chroma_db\",\n",
    "    collection_name=\"my_sample\"\n",
    ")\n",
    "\n",
    "# Delete all documents from the collection\n",
    "vector_store._collection.delete(where={\"id\": {\"$ne\": \"\"}})\n",
    "\n",
    "# adding documents\n",
    "vector_store.add_documents(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7fc1ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = PromptTemplate(\n",
    "            template=\"\"\"You are a query rewriting assistant that helps improve user questions to make them clearer, more specific, and more answerable.\n",
    "\n",
    "            Given the user question, rewrite the question to:\n",
    "\n",
    "            - Use proper grammar and phrasing\n",
    "            - Be more specific if needed\n",
    "            - Align with the provided context\n",
    "            - Remove any ambiguity\n",
    "            - Ensure it helps retrieve or generate a better answer\n",
    "\n",
    "            User Question: {question}\n",
    "            Rewritten Query:\"\"\",\n",
    "            input_variables=['question']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f035edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(retrieved_docs):\n",
    "  context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "  return context_text\n",
    "\n",
    "# Contextual Compression Retrieval\n",
    "base_retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# Set up the compressor using an LLM\n",
    "compressor = LLMChainExtractor.from_llm(llm=model)\n",
    "\n",
    "# Create the contextual compression retriever\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=base_retriever,\n",
    "    base_compressor=compressor\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "template = load_prompt(path='template.json')\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'context': prompt_2 | model | parser | compression_retriever | RunnableLambda(format_docs),\n",
    "    'question': RunnablePassthrough()\n",
    "})\n",
    "\n",
    "chain = parallel_chain | template | model | parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5bfbef02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context you provided, students are categorized into six categories:\\n\\n1. **Category I:**  Students with a CPI of at least 8 and no outstanding grades (FR, DX, DR, W) in a core course.\\n2. **Category II:** Students with a CPI less than 8 and no outstanding grades (FR, DX, DR, W) in a core course.\\n3. **Category III:** Students who have at least one outstanding (FR, DX, DR, W) grade in core courses and may have at most one FR or DX grade in any other course within two preceding regular semesters.  This condition applies as long as they have earned at least 18 credits in each semester. \\n\\n\\nLet me know if you have other questions! \\n'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what are the 6 categories in which students are categorised based on their academic performance\"\n",
    "chain.invoke({'question': query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8dbd7aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "You: Hi\n",
      "Hi! How can I help you today? \n",
      "\n",
      "You: What are the main roles of faculty advisor\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/nebius/v1/chat/completions (Request ID: Root=1-6860342d-408afa3322c4316b3d2c7b4b;3ac70a8d-0bcf-4ec0-ad20-f2f8cc328130)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/requests/models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/nebius/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     13\u001b[0m chat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(msg\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m chat_history)\n\u001b[0;32m---> 14\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m chat_history\u001b[38;5;241m.\u001b[39mappend(AIMessage(content\u001b[38;5;241m=\u001b[39mresult))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_core/runnables/base.py:3045\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3043\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m   3044\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3045\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3046\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3047\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_core/runnables/base.py:3774\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3769\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3770\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3771\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[1;32m   3772\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3773\u001b[0m         ]\n\u001b[0;32m-> 3774\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   3775\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3776\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_core/runnables/base.py:3774\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3769\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3770\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3771\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[1;32m   3772\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3773\u001b[0m         ]\n\u001b[0;32m-> 3774\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   3775\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3776\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_core/runnables/base.py:3758\u001b[0m, in \u001b[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001b[0;34m(step, input_, config, key)\u001b[0m\n\u001b[1;32m   3752\u001b[0m child_config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m   3753\u001b[0m     config,\n\u001b[1;32m   3754\u001b[0m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   3755\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   3756\u001b[0m )\n\u001b[1;32m   3757\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m-> 3758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3760\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_core/runnables/base.py:3047\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3045\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3046\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3047\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3048\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3049\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_core/retrievers.py:259\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 259\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/langchain/retrievers/contextual_compression.py:44\u001b[0m, in \u001b[0;36mContextualCompressionRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_retriever\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m     41\u001b[0m     query, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_manager\u001b[38;5;241m.\u001b[39mget_child()}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m docs:\n\u001b[0;32m---> 44\u001b[0m     compressed_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_compressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(compressed_docs)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/langchain/retrievers/document_compressors/chain_extract.py:73\u001b[0m, in \u001b[0;36mLLMChainExtractor.compress_documents\u001b[0;34m(self, documents, query, callbacks)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[1;32m     72\u001b[0m     _input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_input(query, doc)\n\u001b[0;32m---> 73\u001b[0m     output_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain, LLMChain):\n\u001b[1;32m     75\u001b[0m         output \u001b[38;5;241m=\u001b[39m output_[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_core/runnables/base.py:3047\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3045\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3046\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3047\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3048\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3049\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:372\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    368\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    369\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 372\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    382\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:957\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    955\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    956\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:776\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    775\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 776\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m         )\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1022\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1026\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/langchain_huggingface/chat_models/huggingface.py:574\u001b[0m, in \u001b[0;36mChatHuggingFace._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m     message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    568\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    569\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    570\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    571\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m    572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    573\u001b[0m     }\n\u001b[0;32m--> 574\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(answer)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:924\u001b[0m, in \u001b[0;36mInferenceClient.chat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[1;32m    896\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload_model,\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[1;32m    916\u001b[0m }\n\u001b[1;32m    917\u001b[0m request_parameters \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39mprepare_request(\n\u001b[1;32m    918\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m    919\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    922\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[1;32m    923\u001b[0m )\n\u001b[0;32m--> 924\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:280\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/genai/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:482\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/nebius/v1/chat/completions (Request ID: Root=1-6860342d-408afa3322c4316b3d2c7b4b;3ac70a8d-0bcf-4ec0-ad20-f2f8cc328130)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "source": [
    "# Manual Memory Management\n",
    "chat_history = []\n",
    "print(len(chat_history))\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    print(f\"You: {user_input}\")\n",
    "    chat_history.append(HumanMessage(content=user_input))\n",
    "\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    chat = \"\\n\".join(msg.content for msg in chat_history)\n",
    "    result = chain.invoke({'question': chat})\n",
    "    chat_history.append(AIMessage(content=result))\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8e861cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[SystemMessage(content='', additional_kwargs={}, response_metadata={})]\n",
      "Human: HI\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Assistant: Hey there! ðŸ‘‹  What's up? ðŸ˜Š  \n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[SystemMessage(content='The human starts the conversation with \"HI\". The AI responds with a friendly \"Hey there! ðŸ‘‹  What\\'s up? ðŸ˜Š\". \\n', additional_kwargs={}, response_metadata={})]\n",
      "Human: What is ARP\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Assistant: ARP, or Address Resolution Protocol, is a fundamental networking technology. Imagine you're at a big party where everyone has different addresses, but you need to find your friend who's wearing a bright pink hat.  ARP is like that protocol. \n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "1. **You have your address (MAC address, or physical address of your device)**,  which is unique to your computer or network device like a phone. \n",
      "2. **You want to reach a specific device's IP address (the numerical address, like a phone number),** but you don't have it known in advance.\n",
      "3. **ARP comes into play! It acts like a detailed phonebook (and your device is asking...\"Hey, can you point me to that friend at this party?\")**\n",
      "  * Each device running an ARP packet acts as a receiver. \n",
      "  * It broadcasts these packets to find the 'controller' of the network. \n",
      "  * The network device checks its own list and then sends back the IP address and MAC address. \n",
      "\n",
      "**In short, ARP translates IP addresses into MAC addresses, allowing your device to find its way across the network!**\n",
      "\n",
      "\n",
      "Got it? Anything else I can help you with? ðŸ™‚ \n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[SystemMessage(content='The human starts the conversation with \"HI\". The AI responds with a friendly greeting and asks  \"What\\'s up? ðŸ˜Š\".  The human then inquires about ARP, with the AI describing it as a network technology that acts like a phonebook. The AI explained how ARP works by going through the process of device looking up IP addresses and finding the MAC addresses by broadcasting the ARP packets to a controller in the network.  The AI ended by asking if the human has any other questions. \\n', additional_kwargs={}, response_metadata={})]\n",
      "Human: what is ARP based on the document\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Assistant: ARP is based on the concept of a **network phonebook!** ðŸ“š  \n",
      "\n",
      "Here's how it works:  \n",
      "\n",
      "1. **Discovering IP Addresses:** Your devices have IP addresses (like a unique postal code for your computer). But in the networking world, those IP addresses identify your device differently based on the physical device itself. \n",
      "2. **Bridging the Connection:**To speak properly to each other over a network, devices need to know the MAC addresses (like a unique ID on the physical part of the device). This is where ARP comes in! \n",
      "3. **ARP Packets:** Imagine ARP packets are little letters being sent out. Your device broadcasts these letters, asking, \"Hey everyone, what's the MAC address for [IP address]?\", \n",
      "4. **Finding the MAC Address:** Now, the rest of the network checks the letters and finds the MAC address. It's a bit like forwarding those letters to the right recipient in postal delivery.\n",
      "5. **Connecting the Dots:** Now that the device knows the MAC address for the IP address, it can make a direct connection.\n",
      "\n",
      "Does that make sense? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ConversationSummaryMemory\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# Initialize ConversationSummaryMemory\n",
    "summary_memory = ConversationSummaryMemory(\n",
    "    llm=model,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Setup the conversation chain using summary memory\n",
    "conversation = ConversationChain(\n",
    "    llm=model,\n",
    "    memory=summary_memory,\n",
    "    verbose=True  # Optional: helpful for debugging\n",
    ")\n",
    "\n",
    "# Interactive loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    response = conversation.predict(input=user_input)\n",
    "    print(f\"Assistant: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
